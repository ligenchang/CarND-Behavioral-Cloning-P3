{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8428\n",
      "2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\3point6\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (5, 5), activation=\"relu\", strides=(2, 2), padding=\"valid\")`\n",
      "D:\\ProgramData\\Anaconda3\\envs\\3point6\\lib\\site-packages\\ipykernel_launcher.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(36, (5, 5), activation=\"relu\", strides=(2, 2), padding=\"valid\")`\n",
      "D:\\ProgramData\\Anaconda3\\envs\\3point6\\lib\\site-packages\\ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), activation=\"relu\", strides=(2, 2), padding=\"valid\")`\n",
      "D:\\ProgramData\\Anaconda3\\envs\\3point6\\lib\\site-packages\\ipykernel_launcher.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1), padding=\"valid\")`\n",
      "D:\\ProgramData\\Anaconda3\\envs\\3point6\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1), padding=\"valid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_15 (Lambda)           (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_8 (Cropping2D)    (None, 65, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "lambda_16 (Lambda)           (None, 66, 200, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 31, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 31, 98, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 14, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 14, 47, 36)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 5, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 5, 22, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 3, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 3, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 1, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1, 18, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1164)              1342092   \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               116500    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,595,511\n",
      "Trainable params: 1,595,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\3point6\\lib\\site-packages\\ipykernel_launcher.py:132: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "D:\\ProgramData\\Anaconda3\\envs\\3point6\\lib\\site-packages\\ipykernel_launcher.py:132: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., callbacks=[<keras.ca..., verbose=1, steps_per_epoch=264.375, epochs=100, validation_steps=66.84375)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0327Epoch 00001: val_loss improved from inf to 0.02881, saving model to model.h5\n",
      "265/264 [==============================] - 85s 320ms/step - loss: 0.0327 - val_loss: 0.0288\n",
      "Epoch 2/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0280Epoch 00002: val_loss did not improve\n",
      "265/264 [==============================] - 83s 312ms/step - loss: 0.0280 - val_loss: 0.0291\n",
      "Epoch 3/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0272Epoch 00003: val_loss improved from 0.02881 to 0.02814, saving model to model.h5\n",
      "265/264 [==============================] - 83s 312ms/step - loss: 0.0272 - val_loss: 0.0281\n",
      "Epoch 4/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0263Epoch 00004: val_loss improved from 0.02814 to 0.02752, saving model to model.h5\n",
      "265/264 [==============================] - 83s 314ms/step - loss: 0.0263 - val_loss: 0.0275\n",
      "Epoch 5/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0257Epoch 00005: val_loss did not improve\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0257 - val_loss: 0.0283\n",
      "Epoch 6/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0251Epoch 00006: val_loss improved from 0.02752 to 0.02712, saving model to model.h5\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0251 - val_loss: 0.0271\n",
      "Epoch 7/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0248Epoch 00007: val_loss improved from 0.02712 to 0.02622, saving model to model.h5\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0248 - val_loss: 0.0262\n",
      "Epoch 8/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0240Epoch 00008: val_loss did not improve\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0241 - val_loss: 0.0263\n",
      "Epoch 9/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0238Epoch 00009: val_loss improved from 0.02622 to 0.02513, saving model to model.h5\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0238 - val_loss: 0.0251\n",
      "Epoch 10/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0236Epoch 00010: val_loss improved from 0.02513 to 0.02462, saving model to model.h5\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0236 - val_loss: 0.0246\n",
      "Epoch 11/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0231Epoch 00011: val_loss did not improve\n",
      "265/264 [==============================] - 83s 314ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 12/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0226Epoch 00012: val_loss improved from 0.02462 to 0.02418, saving model to model.h5\n",
      "265/264 [==============================] - 83s 312ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 13/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0224Epoch 00013: val_loss improved from 0.02418 to 0.02394, saving model to model.h5\n",
      "265/264 [==============================] - 83s 312ms/step - loss: 0.0224 - val_loss: 0.0239\n",
      "Epoch 14/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0222Epoch 00014: val_loss improved from 0.02394 to 0.02347, saving model to model.h5\n",
      "265/264 [==============================] - 83s 312ms/step - loss: 0.0222 - val_loss: 0.0235\n",
      "Epoch 15/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0219Epoch 00015: val_loss improved from 0.02347 to 0.02333, saving model to model.h5\n",
      "265/264 [==============================] - 83s 312ms/step - loss: 0.0219 - val_loss: 0.0233\n",
      "Epoch 16/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0211Epoch 00016: val_loss did not improve\n",
      "265/264 [==============================] - 82s 311ms/step - loss: 0.0211 - val_loss: 0.0241\n",
      "Epoch 17/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0211Epoch 00017: val_loss improved from 0.02333 to 0.02316, saving model to model.h5\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0211 - val_loss: 0.0232\n",
      "Epoch 18/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0206Epoch 00018: val_loss improved from 0.02316 to 0.02283, saving model to model.h5\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0206 - val_loss: 0.0228\n",
      "Epoch 19/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0207Epoch 00019: val_loss did not improve\n",
      "265/264 [==============================] - 84s 318ms/step - loss: 0.0207 - val_loss: 0.0236\n",
      "Epoch 20/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00020: val_loss did not improve\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0202 - val_loss: 0.0236\n",
      "Epoch 21/100\n",
      "264/264 [============================>.] - ETA: 0s - loss: 0.0200Epoch 00021: val_loss did not improve\n",
      "265/264 [==============================] - 84s 317ms/step - loss: 0.0200 - val_loss: 0.0233\n",
      "Epoch 00021: early stopping\n",
      "model was saved successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.ndimage import rotate\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Reshape\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.backend import tf as ktf\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#Read csv file to get all data\n",
    "samples = []\n",
    "with open('./img10/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "#split trainging samples and validation samples\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "train_samples = sklearn.utils.shuffle(train_samples)\n",
    "validation_samples = sklearn.utils.shuffle(validation_samples)\n",
    "iglobal=0 # use this for debugging to see how many time generator yields\n",
    "def generator(samples, batch_size=32, validation_flag=True):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            angles = []\n",
    "            global iglobal\n",
    "            iglobal=iglobal+1\n",
    "            #debug to see how many times yields\n",
    "            #print (\"yield\", iglobal, \"times\")\n",
    "            for batch_sample in batch_samples:\n",
    "                for i in range(3):\n",
    "                    name = './img10/IMG/'+batch_sample[i].split('/')[-1]\n",
    "                    #image = cv2.imread(name)\n",
    "                    image=mpimg.imread(name)\n",
    "                    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #image = exposure.equalize_hist(image)\n",
    "                    angle = float(batch_sample[3])\n",
    "                    if i==1: #left camera images \n",
    "                        angle=angle+0.23\n",
    "                    if i==2: #right camera images\n",
    "                        angle=angle-0.23\n",
    "                        \n",
    "                    \n",
    "                    images.append(image)\n",
    "                    angles.append(angle)\n",
    "                    \n",
    "                    if i==9: # this won't be executed as I find the flip image will add lots of noises\n",
    "                        images.append(np.fliplr(image))\n",
    "                        angles.append(angle*-1)\n",
    "                    \n",
    "                    if i==9: #this won't be exucuted das I can't find proper angle adjustment for rorate images\n",
    "                        Rotate_image = rotate(image, 5, reshape=False) \n",
    "                        images.append(Rotate_image)\n",
    "                        angles.append(angle-0.2)\n",
    "                        Rotate_image = rotate(image, -5, reshape=False) \n",
    "                        images.append(Rotate_image)\n",
    "                        angles.append(angle+0.2)\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function, select batch size with 16. If set it with 32, sometimes will encounter OOM exception\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "print(len(train_samples))\n",
    "print(len(validation_samples))\n",
    "def resize_image(image):\n",
    "    # In Nvidia CNN, the image input size is 66 x 200\n",
    "    from keras.backend import tf as ktf   \n",
    "    resized_image = ktf.image.resize_images(image, (66, 200))\n",
    "    return resized_image\n",
    "\n",
    "model=Sequential()\n",
    "#nomalized images\n",
    "model.add(Lambda(lambda x: x /255 - 0.5, input_shape=(160,320,3)))\n",
    "#Cropping the image to only see the middle part and remove the noise\n",
    "model.add(Cropping2D(cropping=((70,25), (0,0))))\n",
    "#REsize the image to Nvidia input size\n",
    "model.add(Lambda(resize_image, input_shape=(65,320,3), output_shape=(66, 200, 3)))\n",
    "\n",
    "#Nvidia 5 lays of CNN with dropout 0.25 on each layer\n",
    "model.add(Convolution2D(24,5,5,border_mode='valid', activation='relu', subsample=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(36,5,5,border_mode='valid', activation='relu', subsample=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(48,5,5,border_mode='valid', activation='relu', subsample=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64,3,3,border_mode='valid', activation='relu', subsample=(1,1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64,3,3,border_mode='valid', activation='relu', subsample=(1,1)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1164, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#use adam optimizer\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "#stop early to prevent overfitting, the tolerance is 2 epoches\n",
    "filepath=\"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# check 2 epochs patience\n",
    "early_stop = EarlyStopping(monitor='val_loss',  patience=3, verbose=1, mode='min') \n",
    "\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "#I set samples_per_epoch as len(train_samples)/batch_size +1 as I think for each epoch, looping through the whole training set is enough\n",
    "model.fit_generator(train_generator, samples_per_epoch= len(train_samples)/32+1, validation_data=validation_generator, nb_val_samples=len(validation_samples)/32+1, nb_epoch=100,callbacks=callbacks_list, verbose=1)\n",
    "#model.save('model.h5')\n",
    "print (\"model was saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
